from pyspark.sql import SparkSession
from pyspark.ml.regression import RandomForestRegressor
from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.evaluation import RegressionEvaluator, MulticlassClassificationEvaluator
from pyspark.ml.tuning import ParamGridBuilder, CrossValidator
from pyspark.sql.functions import log, exp, col
import happybase
import time
import subprocess

def delete_hdfs_model(path):
    """åˆ é™¤ HDFS ä¸Šçš„æ—§æ¨¡å‹"""
    try:
        subprocess.run(['hdfs', 'dfs', '-rm', '-r', path], 
                       stderr=subprocess.DEVNULL, stdout=subprocess.DEVNULL)
        print(f">>> å·²åˆ é™¤æ—§æ¨¡å‹: {path}")
    except:
        pass

# --- åˆå§‹åŒ– ---
spark = SparkSession.builder \
    .appName("Train_Models_With_Performance_Metrics") \
    .enableHiveSupport() \
    .config("spark.driver.memory", "4g") \
    .getOrCreate()
spark.sparkContext.setLogLevel("ERROR")

# è¿æ¥ HBase
print(">>> [ç³»ç»Ÿ] æ­£åœ¨è¿æ¥ HBase...")
connection = happybase.Connection('node01')
connection.open()

# ç¡®ä¿è¡¨å­˜åœ¨
if b'model_metrics' not in connection.tables():
    connection.create_table('model_metrics', {'metrics': dict()})

metrics_table = connection.table('model_metrics')

# ä½¿ç”¨ HDFS è·¯å¾„ï¼ˆåˆ†å¸ƒå¼å­˜å‚¨ï¼‰
base_path = "hdfs:///bigdata_project/models"

print("=" * 80)
print("ğŸš€ å¼€å§‹æ‰§è¡Œæ¨¡å‹ä¼˜åŒ–è®­ç»ƒä»»åŠ¡ (Grid Search + Cross Validation)")
print("=" * 80)

# ==========================================
# Model B: ç¥¨æˆ¿é¢„æµ‹æ¨¡å‹ï¼ˆä¼˜åŒ–ç‰ˆ + æ€§èƒ½è®°å½•ï¼‰
# ==========================================
print("\n>>> [Model B] æ­£åœ¨å‡†å¤‡æ•°æ®...")
start_time_b = time.time()

df_b = spark.sql("""
    SELECT budget, popularity, runtime, revenue 
    FROM movie_db.movies 
    WHERE revenue > 100000 AND budget > 100000 
    AND popularity > 0.1 AND runtime > 60
""")

train_count_b = df_b.count()

# å¯¹æ•°å˜æ¢
df_b = df_b.withColumn("log_budget", log(col("budget"))) \
           .withColumn("log_popularity", log(col("popularity"))) \
           .withColumn("log_revenue", log(col("revenue")))

assembler_b = VectorAssembler(
    inputCols=['log_budget', 'log_popularity', 'runtime'], 
    outputCol='features', handleInvalid="skip"
)
data_b = assembler_b.transform(df_b)
train_b, test_b = data_b.randomSplit([0.8, 0.2], seed=42)

# --- æ ¸å¿ƒä¼˜åŒ–éƒ¨åˆ† ---
print(">>> [Model B] é…ç½®è¶…å‚æ•°ç½‘æ ¼...")
rf_b = RandomForestRegressor(featuresCol='features', labelCol='log_revenue')

param_grid_b = ParamGridBuilder() \
    .addGrid(rf_b.numTrees, [50, 100]) \
    .addGrid(rf_b.maxDepth, [10, 15]) \
    .build()

evaluator_b = RegressionEvaluator(
    labelCol="log_revenue", predictionCol="prediction", metricName="rmse"
)

cv_b = CrossValidator(
    estimator=rf_b,
    estimatorParamMaps=param_grid_b,
    evaluator=evaluator_b,
    numFolds=3,
    parallelism=2
)

print(">>> [Model B] å¼€å§‹äº¤å‰éªŒè¯è®­ç»ƒ (è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ)...")
cv_model_b = cv_b.fit(train_b)
best_model_b = cv_model_b.bestModel

best_trees_b = best_model_b.getNumTrees
best_depth_b = best_model_b.getOrDefault('maxDepth')

# è®¡ç®—æµ‹è¯•é›†æ€§èƒ½
test_rmse_b = evaluator_b.evaluate(best_model_b.transform(test_b))

# è®¡ç®— RÂ² (å†³å®šç³»æ•°)
r2_evaluator = RegressionEvaluator(labelCol="log_revenue", predictionCol="prediction", metricName="r2")
r2_b = r2_evaluator.evaluate(best_model_b.transform(test_b))

training_time_b = time.time() - start_time_b

print(f">>> [Model B] æœ€ä¼˜å‚æ•°: Trees={best_trees_b}, Depth={best_depth_b}")
print(f">>> [Model B] æµ‹è¯•é›† RMSE: {test_rmse_b:.4f}")
print(f">>> [Model B] æµ‹è¯•é›† RÂ²: {r2_b:.4f}")
print(f">>> [Model B] è®­ç»ƒè€—æ—¶: {training_time_b:.2f} ç§’")

# ä¿å­˜æ€§èƒ½æŒ‡æ ‡åˆ° HBase
metrics_table.put(b'model_b', {
    b'metrics:name': 'éšæœºæ£®æ—ç¥¨æˆ¿é¢„æµ‹'.encode('utf-8'),
    b'metrics:rmse': f'{test_rmse_b:.4f}'.encode('utf-8'),
    b'metrics:r2': f'{r2_b:.4f}'.encode('utf-8'),
    b'metrics:trees': str(best_trees_b).encode('utf-8'),
    b'metrics:depth': str(best_depth_b).encode('utf-8'),
    b'metrics:train_samples': str(train_count_b).encode('utf-8'),
    b'metrics:training_time': f'{training_time_b:.2f}'.encode('utf-8')
})
print(">>> [Model B] æ€§èƒ½æŒ‡æ ‡å·²ä¿å­˜åˆ° HBase")

# ä¿å­˜æœ€ä¼˜æ¨¡å‹
model_b_path = f"{base_path}/revenue_model"
delete_hdfs_model(model_b_path)
best_model_b.save(model_b_path)
print(f">>> [Model B] æœ€ä¼˜æ¨¡å‹å·²ä¿å­˜åˆ° {model_b_path}")

# ==========================================
# Model C: æˆè´¥åˆ†ç±»æ¨¡å‹ï¼ˆä¼˜åŒ–ç‰ˆ + æ€§èƒ½è®°å½•ï¼‰
# ==========================================
print("\n>>> [Model C] æ­£åœ¨å‡†å¤‡æ•°æ®...")
start_time_c = time.time()

df_c = spark.sql("""
    SELECT budget, popularity, runtime, revenue 
    FROM movie_db.movies 
    WHERE budget > 100000 AND runtime IS NOT NULL
""")

train_count_c = df_c.count()

df_c = df_c.withColumn("label", (df_c["revenue"] > df_c["budget"]).cast("double"))
df_c = df_c.withColumn("log_budget", log(col("budget"))) \
           .withColumn("log_popularity", log(col("popularity")))

assembler_c = VectorAssembler(
    inputCols=['log_budget', 'log_popularity', 'runtime'], 
    outputCol='features', handleInvalid="skip"
)
data_c = assembler_c.transform(df_c)
train_c, test_c = data_c.randomSplit([0.8, 0.2], seed=42)

# --- æ ¸å¿ƒä¼˜åŒ–éƒ¨åˆ† ---
print(">>> [Model C] é…ç½®è¶…å‚æ•°ç½‘æ ¼...")
rf_c = RandomForestClassifier(featuresCol='features', labelCol='label')

param_grid_c = ParamGridBuilder() \
    .addGrid(rf_c.numTrees, [50, 100]) \
    .addGrid(rf_c.maxDepth, [10, 15]) \
    .build()

evaluator_c = MulticlassClassificationEvaluator(
    labelCol="label", predictionCol="prediction", metricName="accuracy"
)

cv_c = CrossValidator(
    estimator=rf_c,
    estimatorParamMaps=param_grid_c,
    evaluator=evaluator_c,
    numFolds=3,
    parallelism=2
)

print(">>> [Model C] å¼€å§‹äº¤å‰éªŒè¯è®­ç»ƒ...")
cv_model_c = cv_c.fit(train_c)
best_model_c = cv_model_c.bestModel

best_trees_c = best_model_c.getNumTrees
best_depth_c = best_model_c.getOrDefault('maxDepth')

# è®¡ç®—æµ‹è¯•é›†æ€§èƒ½
accuracy = evaluator_c.evaluate(best_model_c.transform(test_c))

# è®¡ç®— F1 åˆ†æ•°
f1_evaluator = MulticlassClassificationEvaluator(
    labelCol="label", predictionCol="prediction", metricName="f1"
)
f1_score = f1_evaluator.evaluate(best_model_c.transform(test_c))

training_time_c = time.time() - start_time_c

print(f">>> [Model C] æœ€ä¼˜å‚æ•°: Trees={best_trees_c}, Depth={best_depth_c}")
print(f">>> [Model C] éªŒè¯é›†å‡†ç¡®ç‡: {accuracy:.4f}")
print(f">>> [Model C] F1 åˆ†æ•°: {f1_score:.4f}")
print(f">>> [Model C] è®­ç»ƒè€—æ—¶: {training_time_c:.2f} ç§’")

# ä¿å­˜æ€§èƒ½æŒ‡æ ‡åˆ° HBase
metrics_table.put(b'model_c', {
    b'metrics:name': 'éšæœºæ£®æ—æˆè´¥åˆ†ç±»'.encode('utf-8'),
    b'metrics:accuracy': f'{accuracy:.4f}'.encode('utf-8'),
    b'metrics:f1': f'{f1_score:.4f}'.encode('utf-8'),
    b'metrics:trees': str(best_trees_c).encode('utf-8'),
    b'metrics:depth': str(best_depth_c).encode('utf-8'),
    b'metrics:train_samples': str(train_count_c).encode('utf-8'),
    b'metrics:training_time': f'{training_time_c:.2f}'.encode('utf-8')
})
print(">>> [Model C] æ€§èƒ½æŒ‡æ ‡å·²ä¿å­˜åˆ° HBase")

# ä¿å­˜æœ€ä¼˜æ¨¡å‹
model_c_path = f"{base_path}/classify_model"
delete_hdfs_model(model_c_path)
best_model_c.save(model_c_path)
print(f">>> [Model C] æœ€ä¼˜æ¨¡å‹å·²ä¿å­˜åˆ° {model_c_path}")

print("\n" + "=" * 80)
print("âœ… æ‰€æœ‰æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
print("=" * 80)

connection.close()
spark.stop()
